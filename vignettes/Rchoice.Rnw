\documentclass[nojss]{jss}

\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amssymb,bm,babel,bbm} % Math packages
\input{ee.sty}
%\VignetteIndexEntry{Discrete choice models with random parameters in R : The Rchoice Package}
%\VignetteDepends{maxLik, Formula, ggplot2, plotrix}
%\VignetteKeywords{discrete choice models, simulated maximum likelihood estimation, R, econometrics}
%\VignettePackage{Rchoice}



%-------------Theorems-----------------------
\usepackage[amsmath,thmmarks,thref,amsthm]{ntheorem}
\usepackage{shadethm}

\newshadetheorem{teo}{Theorem}
\newshadetheorem{defin}{Definition}
\newshadetheorem{assum}{Assumption}
\newshadetheorem{prepo}{Preposition}
\newshadetheorem{lemma}{Lemma}
\newshadetheorem{coro}{Corollary}
\newshadetheorem{exam}{Example}



\title{Discrete choice models with random parameters in \proglang{R} : The \pkg{Rchoice} Package}
\Plainauthor{Mauricio Sarrias}
\author{Mauricio Sarrias\\ Cornell University}

\Plaintitle{Discrete choice models with random parameters in R : The Rchoice Package}

\Keywords{discrete choice models, simulated maximum likelihood,
  \proglang{R}, econometric}

\Plainkeywords{discrete choice models, simulated maximum likelihood estimation,
  R, econometric}
  \Abstract{ \pkg{Rchoice} is a package for \proglang{R} which enables
  the estimation of a variety of Binary, Count and Ordered models with unobserved and observed heterogeneity in the parameters for cross-section data. We implement simulated maximum likelihood methods for the estimation of the coefficients which can assume a variety of distributions such as the \pkg{mlogit} package does. This document is a general description of \pkg{Rchoice} and all functionalities are illustrated using real databases. }


\Address{
Mauricio Sarrias\\
Cornell University\\
Universidad Catolica del Norte\\
E-mail: \email{msarrias86@gmail.com}
\\
}

%% need no \usepackage{Sweave.sty}

\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%

A growing number of empirical studies involve the assessment of influences on a choice among binary, count or ordered discrete alternatives. Those models are well known. However, one of the traditional modeling shortcomings is their inability to control for possible unobserved heterogeneity that may exist across individuals. For instance, one might assume that some variable, such as income or education, does not affect equally the utility of individuals, therefore there may be a deviation from the mean of the respective coefficient. Binary, count and ordered choice models with random parameters extension allows heterogeneity among individuals assuming some distribution on the parameters.\\

In this document we present the package \pkg{Rchoice} for program \proglang{R}. \pkg{Rchoice} is a package for estimating a variety of Ordered and Binary Choice Models with observed and unobserved heterogeneity in the coefficients. The estimation procedure is based on Maximum Simulated Likelihood (MSL) which allows to control for observed and unobserved heterogeneity in a very flexible way. \footnote{For multinomial discrete choice models with random parameters see \pkg{mlogit} package in \proglang{R} \citep{croissant2012estimation}} To our knowledge, only LIMDEP \citep{greene2002limdep} is able to estimate these type of models in a concise and flexible manner. Therefore, this package intends to make available these estimation methods to the general public and practitioners in a friendly and flexible way.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Maximum Simulated Likelihood Estimation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

One of the major capability of the \pkg{Rchoice} package is to estimate Binary, Count and Ordered models with random parameters. They are estimated by \pkg{Rchoice} using Simulated Maximum Likelihood (SML). In this section we briefly explain some basic ideas of SML procedure. For a more complete treatment of SML see for example \cite{gourieroux1997simulation, Hajivassiliou1986, train2009discrete, cameron2005microeconometrics}\\

A random parameter model or random coefficient model permits regression parameter to vary across individuals according to some distribution. A fully parametric random parameter model specifies the dependent variable $y_i$ conditional on regressors $\vx_i$ and given parameters $\vbeta_i$ to have conditional density $f(y_i|\vx_i,\beta_i)$, where $\vbeta_i$ are iid with density $g(\vbeta_i|\vtheta)$ . Inference is based on the density of $y_i$ conditional on $\vx_i$ and given $\vtheta$:

\begin{equation*}
  f(y|\vx_i, \vtheta)=\int f(y|\vx, \vbeta) g(\vbeta, \vtheta) d\vbeta
\end{equation*}

This integral will not have a closed-form solution except in some especial cases. For example, we can assume normally distributed random parameters, with $\vbeta_i\sim N(\vmu, \mSigma)$. Then $\vbeta_i = \vmu + \mSigma^{-1/2}\vupsilon_i$, where $\vupsilon_i\sim N(\vzeros, \mI)$, thus:

\begin{equation}\label{eq:ncfs}
	f(y|\vx,\vtheta)=\int_{-\infty}^{ \infty} f(y|\vx,\mu,\mSigma) \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}\vupsilon^2\right) d\vupsilon 
\end{equation}    

Note that (\ref{eq:ncfs}) has no close-form solution, that is, it is difficult to integrate out the random parameter and hence it difficult to perform ML estimation. However ML estimation may still be possible if we instead use a good approximation $\widehat{f}(y|\vx,\vtheta)$ of $f(y|\vx,\vtheta)$ to form a likelihood function.\\

But, how can we obtain $\widehat{f}(y|\vx,\vtheta)$? A good approximation can be obtained by \textbf{Monte Carlo integration}. \footnote{Another numerical approximation is Gauss-Hermite quadrature. However, it has been documented that for models with more than 3 random parameters SML performs better. } This procedure provides and alternative to deterministic numerical integration. Here we can \emph{simulate} the integration using random draws from the distribution $g(\vbeta|\vtheta)$. For example, the researcher specifies the function form $g(\vbeta|\vtheta)$ and wants to estimate the parameter $\vtheta$. The Monte Carlo approximation is:

\begin{equation*}
\widehat{f}(y|\vx_i,\vbeta_{ir},\vtheta)=\frac{1}{R}\sum_{r=1}^R \tilde{f}(y|\vx,\vbeta_r,\vtheta),
\end{equation*}

where $\vbeta_{ir}$, for example, is the $r$th draw of $\vbeta$ from $g(\vbeta_i|\vtheta)$ for individual $i$. Given independence over $i$, the MSL is the value $\vtheta$ that maximizes:

\begin{equation*}
\widehat{\vtheta}_{MSL}\equiv\underset{\vtheta\in\mTheta}{\arg\max}\quad \sum_{i=1}^N \log \widehat{f}(y|\vx_i,\vbeta_{ir},\vtheta)
\end{equation*}

The following preposition gives the asymptotic distribution of MSL estimator. For a complete derivation of the asymptotic properties of the MSL and a more comprehensive view see \cite{lee1992efficiency} or \cite{gourieroux1997simulation}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{prepo}[Distribution of MSL Estimator]
Assume the following:

\begin{enumerate}
  \item The data are from a simple random sample from a dgp with likelihood function $f(y|\vx,\vtheta_0)$ that satisfies the regularity conditions so that the ML estimator is consistent and asymptotically normal with variance matrix $\mA^{-1}(\vtheta_0)$, where:
	
	\begin{equation*}
		\left[\left.\frac{1}{N}\sum_{i=1}^N\frac{\partial \log f(y|\vx_i,\vtheta)}{\partial \vtheta \partial \vtheta'}\right|_{\vtheta_0}\right]\pto \mA(\vtheta_0)
	\end{equation*}
	
	\item The likelihood function $f$ is estimated using the simulator $\widehat{f}$ with $\tilde{f}$ unbiased for $f$.
\end{enumerate}	
Then the \textbf{maximum simulated likelihood} estimator is asymptotically equivalent to the ML estimator if $R\to \infty$. $N\to\infty$ and $\sqrt{N}/R\to \infty$, and it has a limit normal distribution with:

\begin{equation*}
	\sqrt{N}(\widehat{\vtheta}_{MSL}-\vtheta)\dto N(0,\mA^{-1}(\vtheta_0))
\end{equation*}

\end{prepo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Technical Aspects of Maximum Simulated Likelihood}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, we show the general technical aspect of the MSL. This will allow us to accommodate any index-type regression model such as probit, logit, ordered and count model. This section relies heavily on chapter 15 of \cite{greenebook} and chapter 10 of \cite{train2009discrete}.
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\subsection{Simulated Maximum Likelihood}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In order to develop a general set of results, it is convenient to write each single density in the simulated function as:

\begin{equation*}
  f(y_{i}|\vx_{i}, \vbeta_{ir}, \vtheta)=P_{ir}(\vtheta)=P_{ir}
\end{equation*}

where $\vtheta$ is the vector that collects all the parameters. The simulated log-likelihood is :

\begin{equation}
	\log L_s=\sum_{i=1}^N\log\left(\frac{1}{R} \sum_{r=1}^R P_{ir}(\vtheta) \right) 
\end{equation}

 If we define:

\begin{equation*}
	P_i=P_i(\vtheta)=\frac{1}{R}\sum_{r=1}^RP_{ir},
\end{equation*}

then, the maximum likelihood can be written as:

\begin{equation}
\log L_s=\sum_{i=1}^N\log P_i(\vtheta)
\end{equation}

With this notation, we will be able to accommodate richer specifications of the index function and discrete choice models by simply changing the specification of $P_{ir}$. As typical, the index model represents a latent process of the form:

\begin{equation}\label{eq:index}
U_{ir}^*(\vtheta)=\vz_{i}'\vdelta+\vx_{i}'\vbeta_{ir}+\epsilon_i,
\end{equation}

where $\vz_{i}$ is a vector of variables with fixed parameters $\vdelta$;   $\vx_{i}$ is a vector of variables with random coefficients $\vbeta_{ir}$; and $\epsilon_i$ is the error term. For simplicity, assume that $\vbeta_{ir}\sim N(\vbeta, \mSigma)$, then the random vector of coefficients can be written as:

\begin{equation*}
  \vbeta_{ir}=\vbeta+\mL\vomega_{ir}
\end{equation*}

where $\vomega_{ir}$  is a vector of random draws from normal standard distribution. If the random parameters are correlated normal, then $\mL$ is a lower triangular which produces the covariance matrix of the random parameters, $\mL\mL'=\mSigma$; otherwise, the matrix $\mL$ is a diagonal matrix of standard deviations.  A hierarchical model is obtained by allowing the parameter heterogeneity to be partly systematic in terms of observed variables:

\begin{equation*}
  \vbeta_{ir}=\vbeta+\mPi\vs_i+\mL\vomega_{ir},
\end{equation*}

where $\mPi$ is a matrix of parameters and $\vs_i$ is a vector of covariates. Then, $E(\vbeta_i)=\vbeta+\vpi\vs_i+\mL E(\vomega)=\vbeta+\vpi\vs_i$ and its covariance is $\var(\vbeta_i)=E(\mL\vomega(\vomega\L)')=\mL E(\vomega\vomega')\mL=\mL\mI\mL=\mL\mL'=\mSigma$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{exam}[Representation of Correlated Random Parameters]
Suppose two correlated random parameters, $\beta_1$ and $\beta_2$, whose mean depend upon variables $S, B$ and $C$. Then:

\begin{eqnarray*}
\beta_{1,ir}&=&\bar{\beta}_{1}+\pi_{1,1}S_i+\pi_{1,2}B_i+\pi_{1,3}C_i+s_{11}\omega_{1,ir} \\
\beta_{2,ir}&=&\bar{\beta}_{2}+\pi_{2,1}S_i+\pi_{2,2}B_i+\pi_{2,3}C_i+s_{21}\omega_{1,ir}+s_{22}\omega_{2,ir}
\end{eqnarray*}

or in vector form:

\begin{equation*}
\begin{pmatrix}\beta_{1,ir} \\ \beta_{2,ir} \end{pmatrix}=\begin{pmatrix} \bar{\beta}_{1} \\ \bar{\beta}_{2}\end{pmatrix}+\begin{pmatrix} \pi_{1,1} & \pi_{1,2} & \pi_{1,3}\\ \pi_{2,1} & \pi_{2,2} & \pi_{2,3} \end{pmatrix}\begin{pmatrix} S_i \\ B_i \\ C_i \end{pmatrix}+\begin{pmatrix} s_{11} & 0 \\ s_{21} & s_{22}\end{pmatrix}\begin{pmatrix}\omega_{1,ir} \\ \omega_{2,ir} \end{pmatrix}
\end{equation*}

In this case, the variance-covariance matrix of the random parameters is:

\begin{equation*}
  \mSigma=\mL\mL'=\begin{pmatrix}s_{11} & 0 \\ s_{21} & s_{22}\end{pmatrix}\begin{pmatrix}s_{11} & s_{21} \\ 0 & s_{22}\end{pmatrix}=\begin{pmatrix}s_{11}^2 & s_{11}s_{22} \\ s_{21}s_{22} & s_{21}^2+s_{22}^2\end{pmatrix},
\end{equation*}

and the conditional mean vector is:

\begin{equation*}
    E(\vbeta_i| \vs_i)=\vbeta+\mPi\vs_i
\end{equation*}
\end{exam}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\pkg{Rchoice} allows to specify different distribution for the random parameters. See section 4. \\

Finally, depending in the nature of the dependent variable and the distribution of the error term, the probability for each individual can be specified. It is well known that, if the dependent variable is binary, then the probability for each individual in each draw is:

\begin{equation}\label{eq:probability_binary}
  P_{ir}(\vtheta)=F(q_i\cdot U_{ir}^*(\vtheta))
\end{equation}

where $q_i=2y_i-1$\footnote{As explained by \cite{greenebook}, if the distribution is symmetric, as the normal and logistic are, then $1-F(\vx'\vbeta)=F(\vx'\vbeta)$. Then, $\log L = \sum_i F(q_i\vx_i\vbeta)$ }. Furthermore, if the model is probit, then:

  \begin{eqnarray*}
      F(q_i\cdot U_{ir}^*(\vtheta))&=&\Phi(q_i\cdot U_{ir}^*(\vtheta)) \\
      f(q_i\cdot U_{ir}^*(\vtheta))&=&\phi(q_i\cdot U_{ir}^*(\vtheta)) 
\end{eqnarray*}

where $\Phi(\cdot)$  and $\phi(\cdot)$ are the CDF and the pdf for the standard normal distribution. Likewise, if the model is logit, then:

\begin{eqnarray*}
    F(q_i\cdot U_{ir}^*(\vtheta))&=&\Lambda(q_i\cdot U_{ir}^*(\vtheta))=\frac{\exp(q_i\cdot U_{ir}^*(\vtheta))}{1+\exp(q_i\cdot U_{ir}^*(\vtheta))} \\
    f(q_i\cdot U_{ir}^*(\vtheta))&=&\Lambda(q_i\cdot U_{ir}^*(\vtheta))\left[1-\Lambda(q_i\cdot U_{ir}^*(\vtheta))\right] \\
\end{eqnarray*}


For the Poisson model, the probability for individual $i$ for the $r$ draw is:

\begin{equation}\label{eq:probability_poisson}
  P_{ir}(\vtheta)=\frac{\exp(-\exp(U_{ir}^*(\vtheta)))\exp(U_{ir}^*(\vtheta))^{y_i}}{y_i!}
\end{equation}

and for the ordered model, we have:

\begin{equation}\label{eq:probability_ordered}
  P_{ir}(\vtheta)=F(\kappa_j-U_{ir}^*(\vtheta))-F(\kappa_{j-1}-U_{ir}^*(\vtheta))
\end{equation}

where:

\begin{equation*}
  \kappa_j=\kappa_{j-1}+\exp(\alpha_j)
\end{equation*}

This last reparametrization ensure the ordering of the thresholds.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Gradient of the Simulated Maximum Likelihood}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

SML procedure is very time consuming. Providing the gradient to the maximization procedure can considerable reduce the time to achieve convergence. \pkg{Rchoice} package provides the gradient for all random parameter models. Next, the formulas used by \pkg{Rchoice} to obtain the gradient are given. To obtain the derivatives, we begin with:

\begin{equation}\label{eq:grad1}
	\frac{\partial \log L_s }{\partial \vtheta}=\sum_{i=1}\left( \frac{\frac{1}{R}\sum_{r=1}^R\left( \frac{\partial P_{ir}(\vtheta)}{\partial \vtheta}\right) }{\frac{1}{R}\sum_{r=1}^R P_{ir}(\vtheta)}\right) 
\end{equation}

For the derivative term,


\begin{eqnarray}
	\frac{\partial P_{ir}(\vtheta)}{\partial \vtheta}&=& P_{ir}  \frac{\partial  \log P_{ir}(\vtheta) }{\partial \vtheta} \nonumber \\
	&=& P_{ir}(\vtheta) \vg_{ir}(\vtheta).\label{eq:dprod_pir}
\end{eqnarray}

where we use the fact that $\frac{\partial \log p}{\partial \theta}=\frac{1}{p}\frac{\partial p}{\partial \theta}$. Now, inserting \ref{eq:dprod_pir} into \ref{eq:grad1} we get:

\begin{equation}\label{eq:grad2}
	\frac{\partial \log L_s }{\partial \vtheta}=\sum_{i=1}\left( \frac{\sum_{r=1}^R P_{ir}(\vtheta)\vg_{ir}(\vtheta)}{\sum_{r=1}^RP_{ir}(\vtheta)}\right). 
\end{equation}

Define the weight $Q_{ir}(\vtheta)=P_{ir}(\vtheta)/ \sum_{r=1}^R P_{ir}(\vtheta)$ so that $0<Q_{ir}(\vtheta)<1$ and $\sum_{r=1}^RQ_{ir}(\vtheta)=1$. Then,

\begin{equation}\label{eq:grad3}
	\frac{\partial \log L_s }{\partial \vtheta}=\sum_{i=1}^N\sum_{r=1}^R Q_{ir}(\vtheta)\vg_{ir}(\vtheta)=\sum_{i=1}^N\bar{\vg}_i(\vtheta).
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{exam}[Gradients formulas used by \texttt{Rchoice}]
  For binary models, taking first derivatives on the $\log$ of equation \ref{eq:probability_binary} we get:
  
  \begin{equation}
      \frac{\partial  \log P_{ir}(\vtheta) }{\partial \vtheta}=\vg_{ir}=\lambda_{ir}(\vtheta)\left( \frac{\partial U_{ir}^*(\vtheta)}{\partial \vtheta}\right),
   \end{equation}
   
   where:
   
   \begin{equation*}
      \lambda_{ir}=\frac{q_{i}f(q_{i} U_{ir}^*(\vtheta))}{F(q_{i} U_{ir}^*(\vtheta))},
   \end{equation*}
   
  and $f=\frac{\partial F(\cdot)}{\partial \vtheta}$.  If the model is the Poisson regression model model, then  $\vg_{ir}$ is given by:
  
    \begin{equation*}
      \frac{\partial  \log P_{ir}(\vtheta) }{\partial \vtheta}=\vg_{ir}=(y_i-\exp(U_{ir}^*(\vtheta)))\left( \frac{\partial U_{ir}^*(\vtheta)}{\partial \vtheta}\right).
   \end{equation*}
   
   For the ordered model, let $\vtheta$ the vector collecting all the parameters except for the thresholds parameters. Then, $\vg_{ir}$ is given by:
   
  \begin{equation*}
      \frac{\partial  \log P_{ir}(\vtheta) }{\partial \vtheta}=-\frac{f_{ir,j}(\vtheta)-f_{ir,j-1}(\vtheta)}{F_{ir,j}(\vtheta)-F_{ir,j-1}(\vtheta)}\left( \frac{\partial U_{ir}^*(\vtheta)}{\partial \vtheta}\right),
   \end{equation*}
   
   and by:
   
   \begin{equation*}
  	\frac{\partial \log P_{ir}(\vtheta)}{\partial \valpha_k}=\frac{d_{j,k}f_{ir,j}(\vtheta)-d_{j-1,k}f_{ir,j-1}(\vtheta)}{F_{ir,j}(\vtheta)-F_{ir,j-1}(\vtheta)}\left(\frac{\partial \vkappa_j}{\partial \valpha_k}-\frac{\partial \vkappa_{j-1}}{\partial \valpha_k}\right)
   \end{equation*}
  
  with $\delta_{j,k}=1$ if $j=k$ and 0 otherwise. Finally, if parameters are uncorrelated, then:
  
  \begin{equation*}
\frac{\partial U_{itr}(\vtheta)}{\partial \vtheta}=\begin{pmatrix} \vz_{it} \\ \vx_{it} \\ \vs_{i}\otimes\vx_{it}  \\ \vomega_{it} \bullet \vx_{it}\end{pmatrix}.
\end{equation*}
  
\end{exam}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\pkg{Rchoice} uses this formulas to compute the gradient and uses the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm (as default) to iteratively solve the MSL.


\section{Drawing from Densities}

SML procedure requires to draw pseudo-random numbers from the specified distribution for simulation. A good performance of SML requires very large number of draws. The main drawback to this approach is that with large samples and complex models, the maximization of $\log L_s$ can be very time consuming. Researchers have gained speed with no degradation in simulation performance through the use of small number of Halton draws  \citep{bhat2001quasi, train2000halton}. The idea is that, instead of taking independent random draws, simulation can potentially be improved by selecting evaluation points more systematically and with better coverage \citep{sandor2004quasi}. In this section, we detail how draws are computed by \pkg{Rchoice}. \footnote{In terms of programming, we have modified the functions from \pkg{mlogit} \citep{croissant2012estimation} to allow variation in the mean by observed covariates (Hierarchical model). } \\

Suppose that there are $K_2$ random parameters. Then, the $K_2$ elements of $\vomega_{ir}$ are drawn as follows. We begin with a $K_2$ random vector $\vomega_{ir}$ that is:

\begin{itemize}
  \item $K_2$ independent draws from the standard uniform $(0,1)$ distribution or
  \item $K_2$ independent draws from the $m$th Halton sequence, where $m$ is the $m$th prime number in the sequence of $K_2$ prime numbers beginning with 2.
\end{itemize}

An important attribute of the Halton values is that they are also distributed in the $(0,1)$ interval. Then, the primitive draw (Pseudo or Halton draws) is then transformed to the distribution specified by the user as follows: 

\begin{itemize}
  \item $u_{k,ir}\sim U(0,1)$: primitive draw from halton or pseudo-random number generator
  \item $w_{k,ir}=\Phi^{-1}(u_{k,ir})\sim N(0,1)$
\end{itemize}

Using these two primitive draws, \pkg{Rchoice} creates the random parameters as follows: 


\begin{enumerate}
\item Normal Parameter:

  \begin{eqnarray*}
    \beta_{k,ir}&=&\beta_k+\sigma_kw_{k,ir}\\
        w_{k,ir}&\sim&N(0,1)
    \end{eqnarray*}
    
    where $\beta_k$ and $\sigma_{k}$ are estimated. Then, $\beta_{k,i}\sim N(\vbeta_k,\sigma_k^2)$
    
\item  Truncated normal Parameter:   
 \begin{eqnarray*}
\beta_{k,ir}&=&
  \begin{cases}
  	\beta_k+\sigma_kw_{k,ir} & \mbox{if} \;\;\; \beta_{k,ir}>0 \\
		0 & \mbox{otherwise}
	\end{cases}\\
w_{k,ir}&\sim&N(0,1)
\end{eqnarray*}

where $\beta_k$ and $\sigma_{k}$ are estimated. Then, $\beta_{k,i}\sim N(\vbeta_k,\sigma_k^2)$ with the share below zero massed at zero

\item Log-Normal Distribution:

\begin{eqnarray*}
\beta_{k,ir}&=&\exp\left( \beta_k+\sigma_kw_{k,ir}\right)  \\
w_{k,ir}&\sim&N(0,1)
\end{eqnarray*}

where $\beta_k$ and $\sigma_{k}$ are estimated. Then, $\beta_{k,i}\sim \log N(\beta_k, \sigma_k^2)$

\item Uniform:

\begin{eqnarray*}
	\beta_{k,ir}&=&\beta_k-\sigma_k+2\sigma_ku_{k,ir} \\
	u_{k,ir}&\sim& U(0,1)
\end{eqnarray*}

where $\beta_k$ and $\sigma_{k}$ are estimated. 

\item Triangular distribution:

\begin{eqnarray*}
	\beta_{k,ir}&=&\beta_k+\sigma_kv_{k,ir} \\
	v_{k,ir}&\sim& 1(u_{k,ir}<0.5)\left( \sqrt{2u_{k,ir}}-1\right)+1(u_{k,ir}\geq 0.5)\left( 1-\sqrt{2(1-u_{k,ir})} \right) 
\end{eqnarray*}

where $\beta_k$ and $\sigma_{k}$ are estimated. 

\end{enumerate}



\pkg{Rchoice} allows to the user to specify two type of random draws by the argument \code{haltons}: pseudo-random draws (\code{haltons = NULL}) and Halton draws (\code{haltons = NA}) as default. If \code{haltons = NULL}, the seed is set to \code{set.seed(123)}. The user can change this by the \code{seed} argument. For the Halton draws, the default is to use the first $K_2$ primes numbers starting with 3. Within each series, the first 100 draws are discarded, as the first draws tend to be highly correlated across different draw. The user can also change the prime  number and the element dropped for each serie. For example, if $K_2=2$, and the user wants to use the primes numbers 5 and 31 along with dropping the first 10 draws, he could specify \code{ haltons = list("prime" = c(5,31), "drop" = c(10,10))}. \\   

Note that log-normal and truncated normal give positive coefficients only. If the user wants a variable to have only negative coefficients, he should create the negative of the variable. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Poisson Regression Model Examples}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Standard Poisson Model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In \proglang{R} there exist several package to estimate binary, count and ordered models. \code{glm} function allows to estimate different kind of discrete choice models such as Poisson and binary models. The function \code{probit} from the package \pkg{micEcon} allows to estimate probit model. Moreover, the function {polr} from the package \pkg{MASS} allows to estimate ordered models \citep{venablesbook}. The advantage of 
 \pkg{Rchoice} is that allows more flexibility in the optimization routines which improves the convergence speed.  \pkg{Rchoice} uses the function \code{maxLik} in order to maximize the log-likelihood function, which permits to estimate models by the Newton-Raphson (NR), BGFS and Berndt-Hall-Hall-Hausman (BHHH) procedures \citep[see][]{henningsen2011maxlik}.\\

In this section, we show the capabilities of \pkg{Rchoice} to estimate Poisson regression model with and without random parameters.\\

\pkg{Rchoice} is loaded by typing: 

<<echo=TRUE, message=FALSE>>=
library("Rchoice")
@

In order to show how to estimate Poisson regression models using \pkg{Rchoice}, we will use data on scientific productivity \citep{long1990origins, long1997regression}. We load the data using

<<>>=
data("Articles")
head(Articles,3)
@

To see more information about the data, one can use:

<<>>=
help(Articles)
@

The work by \cite{long1990origins} suggest that gender, marital status, number of young children, prestige of the graduate program, and the number of articles written by a scientist's mentor could affect a scientist's level of publication. In order to see this, we estimate a Poisson regression model use the \code{Rchoice} function specifying \code{link = "poisson"}:

<<tidy=FALSE>>=
poisson<-Rchoice(art ~ fem + mar + kid5 + phd + ment, data = Articles, 
                 link = "poisson")
summary(poisson)
@

The output shows that the log-likelihood function is estimated using NR algorithm in 7 iterations. If the user wants to estimate the model using another algorithm he should type \code{method = "bfgs"} for the BGFS method or \code{method = "bhhh"} for BHHH method.\\

In terms of interpretation, we can say that, being a female scientist decreases the expected number of articles by a factor of 0.8 $(=\exp(-.225))$, holding all other variables constant. Or equivalently, being a female scientist decreases the expected number of articles by 20\% $(=100\left[\exp(-.225)-1\right])$, holding all other variables constant. Prestige of PhD department is not important for productivity. \\

Another capability of \pkg{Rchoice} is its interaction with other packages in \proglang{R}. For example, we can compute the robust standard error by using the package \pkg{sandwich}:

<<echo=TRUE, message=FALSE>>=
require(sandwich)
require(lmtest)
@

<<>>=
coeftest(poisson, vcov = sandwich)
@

In order to get the same robust standard errors as STATA \citep{stata12}, we need to make a small sample correction:

<<>>=
vcov.stata <- vcovHC(poisson, type = "HC0") * nObs(poisson)/(nObs(poisson)-1)
coeftest(poisson, vcov = vcov.stata)
@

where the correction is $n/(n-1)$. \\

\pkg{Rchoice} also interacts with \code{linearHypothesis} and \code{deltaMethod} functions from \pkg{car} \citep{fox2009car} and the  \code{lrtest} and \code{waldtest} functions from \pkg{lmtest} package \citep{zeileisR}. For example, we can test $H_0: \texttt{phd}/\texttt{ment}=0$ by: 

<<echo=TRUE, message=FALSE>>=
require(car)
@

<<>>=
deltaMethod(poisson, "phd/ment")
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Random Parameter Poisson Model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Now, we estimate a Poisson regression model with random parameters. In this case, we will assume that the effect of \texttt{kid5}, \texttt{phd} and \texttt{ment} are not fixed, but rather heterogeneous among the population. Specifically, we will assume that the coefficients for those variables are independent normally distributed, that is, we will not allow correlation among them:

\begin{eqnarray*}
 \beta_{\texttt{kid5},i}&=&\beta_{\texttt{kid5}}+\sigma_{\texttt{kid5}}\omega_{\texttt{kid5},ir} \\
 \beta_{\texttt{phd},i}&=&\beta_{\texttt{phd}}+\sigma_{\texttt{phd}}\omega_{\texttt{phd},ir} \\
 \beta_{\texttt{ment},i}&=&\beta_{\texttt{ment}}+\sigma_{\texttt{ment}}\omega_{\texttt{ment},ir}   
\end{eqnarray*}

Then, in order to estimate this model, we can write:

<<tidy=FALSE>>=
poisson.ran <- Rchoice(art ~ fem + mar + kid5 + phd + ment, 
                       data = Articles, ranp = c(kid5="n", phd = "n", ment = "n"), 
                       link = "poisson")
@


It is important to discuss the arguments for the \code{Rchoice} function. First, the argument \code{ranp} indicates which variables are random in the \code{formula} and their distributions. In this case, we have specified that all of them are normal distributed using \code{"n"}. The number of draws are not specified. Therefore, \code{Rchoice} will set \code{R = 40} as default. The user can change this by changing the \code{R} argument. The type of draws are Halton draws as a default, but if the user wants pseudo-random draws he can specify \code{haltons = NULL}. As explained before, the default maximization algorithm for SML is BGFS. 

<<>>=
summary(poisson.ran)
@

The result shows that indeed the three coefficients are random in the sample. We can also say that : 

<<>>=
pnorm(coef(poisson.ran)["mean.kid5"]/coef(poisson.ran)["sd.kid5"])
@

a 23\% of the individuals have a positive coefficient for \texttt{kid5}. Note also that the mean coefficient for \texttt{phd} is 0 (not significant). This is due to the fact that the unobserved heterogeneity among scientists in the sample cancel out positive and negative effects. These observations are not possible with a Poisson regression with fixed effect. \\


Suppose that now we want to test if $H_0=\sigma_{\texttt{kid5}}=\sigma_{\texttt{phd}}=\sigma_{\texttt{ment}}=0$. This can be done by using the function \code{waldtest} or \code{lrtest} from package \pkg{lmtest}:

<<>>=
waldtest(poisson.ran, poisson)
lrtest(poisson.ran, poisson)
@

Both test reject the null hypothesis. We can also specify different distribution of the parameters by using the S3 method \code{update}:

<<>>=
poisson.ran2 <- update(poisson.ran, ranp = c(kid5 = "u", phd = "t" , ment = "cn"))
summary(poisson.ran2)
@

Now, we estimate the model \texttt{poisson.ran}, but assuming that the random parameters are correlated:

<<tidy=FALSE>>=
poissonc.ran <- Rchoice(art ~ fem + mar + kid5 + phd + ment, data = Articles, 
                        ranp = c(kid5="n", phd = "n", ment = "n"), 
                        link = "poisson", correlation =  TRUE)
summary(poissonc.ran)
@

We can extract the $\mSigma=\mL\mL'$ matrix of variance-covariance matrix and the correlation matrix of the random parameters using \code{cov.Rchoice} and \code{cor.Rchoice}:

<<>>=
cov.Rchoice(poissonc.ran)
cor.Rchoice(poissonc.ran)
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hierarchical Poisson Random Parameter Model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section we show how to estimate a Hierarchical Poisson Random Parameter Model. In this case, we assume that there exist not only unobserved heterogeneity in the coefficients for \texttt{kid5}, \texttt{phd} and \texttt{ment}, but also observed heterogeneity in the mean. Specifically, we assume that:

\begin{eqnarray*}
 \beta_{\texttt{kid5},i}&=&\beta_{\texttt{kid5}}+\pi_{1}\texttt{fem}+\sigma_{\texttt{kid5,kid5}}\omega_{\texttt{kid5},ir} \\
 \beta_{\texttt{phd},i}&=&\beta_{\texttt{phd}}+\pi_{2}\texttt{fem}+\sigma_{\texttt{phd,kid5}}\omega_{\texttt{kid5},ir} + \sigma_{\texttt{phd,phd}}\omega_{\texttt{phd},ir} \\
 \beta_{\texttt{ment},i}&=&\beta_{\texttt{ment}}+\pi_{3}\texttt{fem}+\sigma_{\texttt{ment,kid5}}\omega_{\texttt{kid5},ir}+\sigma_{\texttt{ment,phd}}\omega_{\texttt{phd},ir}+\sigma_{\texttt{ment,ment}}\omega_{\texttt{ment},ir}   
\end{eqnarray*}

The formulation above implies that those three coefficients (or marginal effect on latent productivity) varies also if the individual is female or male. \pkg{Rchoice} manages the variables in the hierarchical model by the \code{formula} object: all the hierarchical variables are included after the \code{|} symbol. For example, we can estimate this model by typing:  

<<tidy=FALSE>>=
poissonH.ran <- Rchoice(art ~ fem + mar + kid5 + phd + ment | fem, 
                        data = Articles,
                        ranp = c(kid5="n", phd = "n", ment = "n"), 
                        link = "poisson",
                        correlation =  TRUE)
summary(poissonH.ran)
@

The estimated parameters indicates that gender matters only for \texttt{phd} mean coefficient. We can test if the interaction variables are jointly significant by using \code{lrtest}:

<<>>=
lrtest(poissonH.ran, poissonc.ran)
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Plotting Conditional Means}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

It is important to note that the estimates of the model parameters provide the unconditional estimates of the parameter vector, but we can form a person specific conditional estimator \citep[see][]{train2009discrete, greenebook}. The estimator of the conditional mean of the distribution of the random parameters, conditioned on the person specific data, is:

\begin{equation*}
\widehat{\bar{\vbeta_i}}=\widehat{E}(\vbeta_i|\mbox{data}_i)=\sum_{r=1}^R\widehat{Q}_{ir}\widehat{\vbeta}_{ir}
\end{equation*}
where:

\begin{equation*}
  \widehat{\vbeta}_{ir}=\widehat{\vbeta}+\widehat{\mPi}\vs_i+\widehat{\mL}\vomega_{ir}
\end{equation*}

We can also estimate the standard deviation of this distribution by estimating:

\begin{equation*}
\widehat{E}(\vbeta_i^2|\mbox{data}_i)=\sum_{r=1}^R\widehat{Q}_{ir}\widehat{\vbeta}_{ir}^2,	
\end{equation*}

then computing the square root of the estimated variance,

\begin{equation*}
	\sqrt{\widehat{E}(\vbeta_i^2|\mbox{data}_i)-\widehat{E}(\vbeta_i|\mbox{data}_i)^2}
\end{equation*}

With the estimates of the conditional mean and conditional variance, we can then compute the limits of an interval that resembles a confidence interval as the mean plus and minus two estimated standard deviation. This will construct an interval that contains at least 95 percent of the conditional distribution of $\vbeta_i$ \citep{greenebook}.\\ 

\pkg{Rchoice} allows to plot the histogram and kernel density of conditional means of random parameters using the function \code{plot}. For the histogram of the conditional mean of $\beta_{\texttt{ment},i}$, we can write:

<<fig=TRUE,fig.width=7, fig.height=5,echo=TRUE>>=
plot(poissonH.ran, par = "ment", type = "histogram", bin=0.005)
@

and for the kernel density:

<<fig=TRUE,fig.width=7, fig.height=5,echo=TRUE>>=
plot(poissonH.ran, par = "ment")
@

As \cite{greenebook} points out, even if the analysis departs from normal marginal distributions $\vbeta_i$, the sample distribution of the $n$ estimated conditional means is not necessarily normal. Therefore, the kernel estimator based on the $n$ estimators can have a variety of shapes.\\

We may also plot the individual confident interval for the conditional means for the first 50 individuals:

<<fig=TRUE,fig.width=7, fig.height=5,echo=TRUE>>=
plot(poissonH.ran, par = "ment", ind = TRUE, id = seq(1, 50, 1))
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Binary and Ordered Model Examples}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, we show how to estimate Binary and Ordered (Probit or Logit) models. Since the main characteristics of \pkg{Rchoice} package to estimate random parameter models were shown in the previous section, in this section we briefly show how to use \pkg{Rchoice} for Binary and Ordered models with random parameters.\\


MSL procedure is very complex. Sometimes, the model may not converge, or it may converge to a local maximum, if the initial values are not chosen well enough. To illustrate this, we will estimate a Probit model with random parameters using \code{"Workroz"} database. In order to see the maximization process, we use the argument \code{print.level}.

<<>>=
data("Workmroz")
@


<<tidy=FALSE>>=
probit.ran <- Rchoice(lfp ~ k5 + k618 + age + wc + hc + lwg + inc, 
                      ranp = c(k5 = "n", hc = "n"), 
                      link = "probit", 
                      print.level = 1, 
                      data = Workmroz, 
                      R = 100)
summary(probit.ran)
@

The output for the maximization shows that the model converges with 1 iteration, but some \code{NaNs} are produced when trying to get the standard errors. In order to avoid this problem, the user may provide different initial values using the argument \code{start}. \footnote{As default, the initial values correspond to those for a Probit model.} For example, if we set the initial values all zeros, we get:

<<>>=
probit2.ran <- update(probit.ran, start = c(0,0,0,0,0,0,0,0,0,0))
summary(probit2.ran)
@


In order to analyze ordered models, we use the \code{"Health"} database:

<<>>=
data("Health")
@

We estimate a Random Parameter Ordered Probit model for the variable \texttt{newhsat} for year 1988 (see \texttt{help(Health)}) 

<<tidy=FALSE>>=
oprobit.ran<-Rchoice(newhsat ~ age + educ + hhinc + married + hhkids, 
                      data = Health, link = "ordered probit", 
                      subset = year == 1988, 
                      ranp = c(age = "n", hhinc = "n"), print.level=1, 
                      start = rep(0,11))
summary(oprobit.ran)
@


\pagebreak
\bibliography{Rchoice.bib}


\end{document}