\documentclass[nojss]{jss}

%% Some packages
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amssymb,bm,babel,bbm} % Math packages
\usepackage{graphicx}
\usepackage{float} 
\input{ee.sty}
%\VignetteIndexEntry{Discrete choice models with random parameters in R: The Rchoice Package}
%\VignetteEngine{knitr::knitr}
%\VignetteDepends{maxLik, Formula}
%\VignetteKeywords{discrete choice models, simulated maximum likelihood estimation, R, econometrics}
%\VignettePackage{Rchoice}



%-------------Theorems-----------------------
\usepackage[amsmath,thmmarks,thref,amsthm]{ntheorem}
\usepackage{shadethm}

\newshadetheorem{teo}{Theorem}
\newshadetheorem{defin}{Definition}
\newshadetheorem{assum}{Assumption}
\newshadetheorem{prepo}{Preposition}
\newshadetheorem{lemma}{Lemma}
\newshadetheorem{coro}{Corollary}
\newshadetheorem{exam}{Example}


%==================== Plain ============================

% Author
\author{Mauricio Sarrias\\ Cornell University}
\Plainauthor{Mauricio Sarrias}

% Title
\title{Discrete Choice Models with Random Parameters in \proglang{R}: The \pkg{Rchoice} Package}
\Plaintitle{Discrete Choice Models with Random Parameters in R: The Rchoice Package}
\Shorttitle{\pkg{Rchoice} Package in R}

\Keywords{discrete choice models, simulated maximum likelihood,
  \proglang{R}, econometric, random parameters, hierarchical models}

\Plainkeywords{discrete choice models, simulated maximum likelihood estimation,
  R, econometric}
  \Abstract{ \pkg{Rchoice} is a package for \proglang{R} which enables
  the estimation of a variety of Binary, Count and Ordered models with unobserved and observed heterogeneity in the parameters for cross-sectional and panel data. We implement simulated maximum likelihood methods for the estimation of the coefficients which can assume a variety of distributions. This document is a general description of \pkg{Rchoice} and all functionalities are illustrated using real databases. }


\Address{
Mauricio Sarrias \\
325 W. Sibley Hall \\
Department of City and Regional Panning \\
Cornell University \\
E-mail: \email{msarrias86@gmail.com} \\
URL: \url{http://www.msarrias.weebly.com}
\\
}

%% need no \usepackage{Sweave.sty}

\begin{document}

<<echo = FALSE, include =  FALSE>>=
# set global chunk options
library(knitr)
opts_chunk$set(tidy = FALSE, fig.width = 7, fig.height = 5, comment = "")
@

%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%

A growing number of empirical studies seek to measure the impact of covariates on nominal discrete alternatives. Those models are well known. However, one of the traditional modeling shortcomings is their inability to control for possible observed or unobserved heterogeneity that may exist across individuals. For instance, one might assume that some variable does not affect equally the utility of individuals, therefore there may be a deviation from the mean of the respective coefficient. This type of modelling has been widely used in Multinomial Logit Model known also as Mixed Logit Model \citep[see][]{train2009discrete, hensher2003mixed} and can be estimated in \proglang{R} using \pkg{mlogit} \citep{croissant2012estimation}. \\

Observed and unobserved heterogeneity can easily be extended to binary, ordered and count data \citep[see][]{gourieroux1984pseudo, ng2006estimation, greene2007functional, greenebook, greene2010modeling, greene2010ordered}. Allowing parameter values to vary across the population according to some pre-specified distribution overcomes the problem of having a fixed-representative coefficient for all individuals. Furthermore, one might assume that this heterogeneity is not only due to unobserved factors, but also to observed individual characteristics such as socioeconomic variables.\\

In this document we present the package \pkg{Rchoice} for \proglang{R}. \pkg{Rchoice} is a package for estimating ordered, count and binary choice models with observed and unobserved heterogeneity in the coefficients. The estimation procedure is based on Simulated Maximum Likelihood (SML) which allow controlling for observed and unobserved heterogeneity in a very flexible way. To our knowledge, only \proglang{LIMDEP} \citep{greene2002limdep} is able to estimate these type of models in a concise and flexible manner. Therefore, this package is intended to make  these estimation methods available to the general public and practitioners in a friendly and flexible way.\\ 

This new version of \pkg{Rchoice} (version 0.2) allows estimating Panel Data models and makes the inclusion of hierarchical variables more flexible. It also includes new helper functions to get the standard errors of the variance-covariance matrix of the random parameters. \\

This paper is organized as follows. In section 2 we briefly explain the Simulated Maximum Likelihood Procedure. Section 3 we discuss some technical aspects of  Simulated Maximum Likelihood estimations and the formulas used by \pkg{Rchoice}. In Section 4 we explain how \pkg{Rchoice} handles creates the random coefficients. Section 5 we show the all functionalities of \pkg{Rchoice} are illustrated using real databases. Section 6 concludes. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Simulated Maximum Likelihood Estimation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section we briefly explain some basic ideas of SML procedure. For a more complete treatment of SML see for example \cite{gourieroux1997simulation, lee1992efficiency, Hajivassiliou1986} or \cite{train2009discrete}.\\

A random parameter model or random coefficient model permits regression parameter to vary across individuals according to an arbitrarily specified distribution. A fully parametric random parameter model specifies the dependent variable $y_i$ conditional on regressors $\vx_i$ and given parameters $\vbeta_i$ to have conditional density $f(y_i|\vx_i,\vbeta_i)$, where $\vbeta_i$ are iid with density $g(\vbeta_i|\vtheta)$. Inference is based on the density of $y_i$ conditional on $\vx_i$ and given $\vtheta$:

\begin{equation*}
  f(y|\vx_i, \vtheta)=\int f(y|\vx, \vbeta) g(\vbeta, \vtheta) d\vbeta
\end{equation*}

This integral will not have a closed-form solution except in some special cases. For example, we can assume normally distributed random parameters, with $\vbeta_i\sim N(\vmu, \mSigma)$. Then $\vbeta_i = \vmu + \mSigma^{-1/2}\vupsilon_i$, where $\vupsilon_i\sim N(\vzeros, \mI)$, thus:

\begin{equation}\label{eq:ncfs}
	f(y|\vx,\vtheta)=\int_{-\infty}^{ \infty} f(y|\vx,\mu,\mSigma) \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}\vupsilon^2\right) d\vupsilon 
\end{equation}    

Note that (\ref{eq:ncfs}) has no close-form solution, that is, it is difficult to integrate out the random parameter and hence it difficult to perform ML estimation. However ML estimation may still be possible if we instead use a good approximation $\widehat{f}(y|\vx,\vtheta)$ of $f(y|\vx,\vtheta)$ to form a likelihood function.\\

But, how can we obtain $\widehat{f}(y|\vx,\vtheta)$? A good approximation can be obtained by \textbf{Monte Carlo integration}.\footnote{Another numerical approximation is Gauss-Hermite quadrature. However, it has been documented that for models with more than 3 random parameters SML performs better. } This procedure provides and alternative to deterministic numerical integration. Here we can `\emph{simulate}' the integration using random draws from the distribution $g(\vbeta|\vtheta)$. For example, the researcher specifies the function form $g(\vbeta|\vtheta)$ and wants to estimate the parameter $\vtheta$. The Monte Carlo approximation is:

\begin{equation*}
\widehat{f}(y|\vx_i,\vbeta_{i},\vtheta)=\frac{1}{R}\sum_{r=1}^R \tilde{f}(y|\vx,\vbeta_{ir},\vtheta),
\end{equation*}

where $\vbeta_{ir}$ is the $r$th draw of $\vbeta$ from $g(\vbeta_i|\vtheta)$ for individual $i$. Given independence over $i$, the SML is the value $\vtheta$ that maximizes:

\begin{equation*}
\widehat{\vtheta}_{SML}\equiv\underset{\vtheta\in\mTheta}{\arg\max}\quad \sum_{i=1}^N \log \widehat{f}(y|\vx_i,\vbeta_{ir},\vtheta)
\end{equation*}

The following preposition gives the asymptotic distribution of SML estimator. For a complete derivation of the asymptotic properties of the SML and a more comprehensive view see \cite{lee1992efficiency} or \cite{gourieroux1997simulation}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{prepo}[Distribution of SML Estimator]
Assume the following:

\begin{enumerate}
  \item The data are from a simple random sample from a dgp with likelihood function $f(y|\vx,\vtheta_0)$ that satisfies the regularity conditions so that the ML estimator is consistent and asymptotically normal with variance matrix $\mA^{-1}(\vtheta_0)$, where:
	
	\begin{equation*}
		\left[\left.\frac{1}{N}\sum_{i=1}^N\frac{\partial \log f(y|\vx_i,\vtheta)}{\partial \vtheta \partial \vtheta'}\right|_{\vtheta_0}\right]\pto \mA(\vtheta_0)
	\end{equation*}
	
	\item The likelihood function $f$ is estimated using the simulator $\widehat{f}$ with $\tilde{f}$ unbiased for $f$.
\end{enumerate}	
Then the \textbf{simulated maximum likelihood} estimator is asymptotically equivalent to the ML estimator if $R\to \infty$. $N\to\infty$ and $\sqrt{N}/R\to \infty$, and it has a limit normal distribution with:

\begin{equation*}
	\sqrt{N}(\widehat{\vtheta}_{SML}-\vtheta)\dto N(0,\mA^{-1}(\vtheta_0))
\end{equation*}

\end{prepo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Technical Aspects of Simulated Maximum Likelihood}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, we show the general technical aspect of the SML. This will allow us to accommodate any index-type regression model such as Probit, Logit, Ordered and Poisson model. This section relies heavily on chapter 15 of \cite{greenebook} and chapter 10 of \cite{train2009discrete}.
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\subsection{Simulated Maximum Likelihood}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In order to develop a general set of results, it is convenient to write each single density in the simulated function as:

\begin{equation*}
  f(y_{it}|\vx_{it}, \vbeta_{ir}, \vtheta)=P_{itr}(\vtheta)
\end{equation*}

where $\vtheta$ is the vector that collects all the parameters. The simulated log-likelihood is :

\begin{equation}
	\log L_s=\sum_{i=1}^N\log\left(\frac{1}{R} \sum_{r=1}^R \prod_{t = 1}^{T_i}P_{itr}(\vtheta) \right) 
\end{equation}

 If we define:

\begin{equation*}
	P_{ir}(\vtheta)=\prod_{t = 1}^{T_i}P_{itr}(\vtheta),
\end{equation*}

and:

\begin{equation*}
  P_{i}(\vtheta)=\frac{1}{R}\sum_{r = 1}^R P_{ir}(\vtheta),
\end{equation*}


then, the maximum likelihood can be written as:

\begin{equation}
\log L_s=\sum_{i=1}^N\log P_i(\vtheta)
\end{equation}

With this notation, we will be able to accommodate richer specifications of the index function and discrete choice models by simply changing the specification of $P_{itr}(\vtheta)$. As is typically the case, the index model represents a latent process of the form:

\begin{equation}\label{eq:index}
U_{itr}^*(\vtheta)=\vz_{it}'\vdelta+\vx_{it}'\vbeta_{ir}+\epsilon_{it},
\end{equation}

where $\vz_{it}$ is a vector of variables with fixed parameters $\vdelta$; $\vx_{it}$ is a vector of variables with random coefficients $\vbeta_{ir}$; and $\epsilon_{it}$ is the error term. For simplicity, assume that $\vbeta_{i}\sim N(\vbeta, \mSigma)$, then the random vector of coefficients can be written as:

\begin{equation*}
  \vbeta_{ir}=\vbeta+\mL\vomega_{ir}
\end{equation*}

where $\vomega_{ir}$  is a vector of random draws from normal standard distribution. If the random parameters are correlated normal, then $\mL$ is a lower triangular which produces the covariance matrix of the random parameters, $\mL\mL'=\mSigma$; otherwise, the matrix $\mL$ is a diagonal matrix of standard deviations. The random effect model is a special case in which only the constant is random.  A hierarchical model is obtained by allowing the parameter heterogeneity to be partly systematic in terms of observed variables:

\begin{equation*}
  \vbeta_{ir}=\vbeta+\mPi\vs_i+\mL\vomega_{ir},
\end{equation*}

where $\mPi$ is a matrix of parameters and $\vs_i$ is a vector of covariates that do not vary across time. \footnote{Note that the \cite{mundlak1978pooling} and \cite{chamberlain1980analysis} approach to modelling fixed effects is also accommodated by letting $\vs_i = \bar{\vx}_i$, in the equation for the overall consant term \citep{greene2010ordered}} Then, $E(\vbeta_i)=\vbeta+\mPi\vs_i+\mL E(\vomega)=\vbeta+\mPi\vs_i$ and its covariance is $\var(\vbeta_i)=E(\mL\vomega(\vomega\mL)')=\mL E(\vomega\vomega')\mL=\mL\mI\mL=\mL\mL'=\mSigma$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{exam}[Representation of Correlated Random Parameters]
Suppose two correlated random parameters, $\beta_1$ and $\beta_2$, whose means depend upon variables $S, B$ and $C$. Then:

\begin{eqnarray*}
\beta_{1,ir}&=&\bar{\beta}_{1}+\pi_{1,1}S_i+\pi_{1,2}B_i+\pi_{1,3}C_i+s_{11}\omega_{1,ir} \\
\beta_{2,ir}&=&\bar{\beta}_{2}+\pi_{2,1}S_i+\pi_{2,2}B_i+\pi_{2,3}C_i+s_{21}\omega_{1,ir}+s_{22}\omega_{2,ir}
\end{eqnarray*}

or in vector form:

\begin{equation*}
\begin{pmatrix}\beta_{1,ir} \\ \beta_{2,ir} \end{pmatrix}=\begin{pmatrix} \bar{\beta}_{1} \\ \bar{\beta}_{2}\end{pmatrix}+\begin{pmatrix} \pi_{1,1} & \pi_{1,2} & \pi_{1,3}\\ \pi_{2,1} & \pi_{2,2} & \pi_{2,3} \end{pmatrix}\begin{pmatrix} S_i \\ B_i \\ C_i \end{pmatrix}+\begin{pmatrix} s_{11} & 0 \\ s_{21} & s_{22}\end{pmatrix}\begin{pmatrix}\omega_{1,ir} \\ \omega_{2,ir} \end{pmatrix}
\end{equation*}

In this case, the variance-covariance matrix of the random parameters is:

\begin{equation*}
  \mSigma=\mL\mL'=\begin{pmatrix}s_{11} & 0 \\ s_{21} & s_{22}\end{pmatrix}\begin{pmatrix}s_{11} & s_{21} \\ 0 & s_{22}\end{pmatrix}=\begin{pmatrix}s_{11}^2 & s_{11}s_{22} \\ s_{21}s_{22} & s_{21}^2+s_{22}^2\end{pmatrix},
\end{equation*}

and the conditional mean vector is:

\begin{equation*}
    E(\vbeta_i| \vs_i)=\vbeta+\mPi\vs_i
\end{equation*}
\end{exam}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Finally, depending on the nature of the dependent variable and the distribution of the error term, the probability of a given outcome can be specified. It is well known that, if the dependent variable is binary, then the probability for each individual in each draw is:

\begin{equation}\label{eq:probability_binary}
  P_{itr}(\vtheta)=F\left[q_{it}\cdot U_{itr}^*(\vtheta)\right]
\end{equation}

where $q_{it}=2y_{it}-1$.\footnote{As explained by \cite{greenebook}, if the distribution is symmetric, as the normal and logistic are, then $1-F(\vx'\vbeta)=F(\vx'\vbeta)$. Then, $\log L = \sum_i F(q_i\vx_i\vbeta)$ } Furthermore, if the model is Probit, then:

  \begin{eqnarray*}
      F\left[q_{it}\cdot U_{itr}^*(\vtheta)\right]&=&\Phi\left[q_{it}\cdot U_{itr}^*(\vtheta)\right] \\
      f\left[q_{it}\cdot U_{itr}^*(\vtheta)\right]&=&\phi\left[q_{it}\cdot U_{itr}^*(\vtheta)\right] 
\end{eqnarray*}

where $\Phi(\cdot)$  and $\phi(\cdot)$ are the CDF and the pdf for the standard normal distribution. Likewise, if the model is logit, then:

\begin{eqnarray*}
    F\left[q_{it}\cdot U_{itr}^*(\vtheta)\right] &=&\Lambda\left[q_{it}\cdot U_{itr}^*(\vtheta)\right] =\frac{\exp(q_{it}\cdot U_{itr}^*(\vtheta))}{1+\exp(q_{it}\cdot U_{itr}^*(\vtheta))} \\
    f\left[q_{it}\cdot U_{itr}^*(\vtheta)\right] &=&\Lambda\left[q_{it}\cdot U_{itr}^*(\vtheta)\right] \left[1-\Lambda(q_{it}\cdot U_{itr}^*(\vtheta))\right] \\
\end{eqnarray*}


For the Poisson model, the probability for individual $i$ and the $r$th draw is:

\begin{equation}\label{eq:probability_poisson}
  P_{itr}(\vtheta)=\frac{\exp(-\exp(U_{itr}^*(\vtheta)))\exp(U_{itr}^*(\vtheta))^{y_{it}}}{y_{it}!}
\end{equation}

and for the Ordered model, we have:

\begin{equation}\label{eq:probability_ordered}
  P_{itr}(\vtheta)=F\left[\kappa_j-U_{itr}^*(\vtheta)\right] -F\left[\kappa_{j-1}-U_{itr}^*(\vtheta)\right] 
\end{equation}

where:

\begin{equation*}
  \kappa_j=\kappa_{j-1}+\exp(\alpha_j)
\end{equation*}

This last reparametrization ensures the ordering of the thresholds.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Gradient of the Simulated Maximum Likelihood}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The SML procesure is very time consuming. If the model is very complex, the algorithm may take hours to converge. Providing the gradient to the maximization procedure can considerable reduce the time to achieve convergence. The \pkg{Rchoice} package provides the gradient for all random parameter models. \\

Next, the formulas used by \pkg{Rchoice} to obtain the gradient are given. To obtain the derivatives, we begin with:

\begin{equation}\label{eq:grad1}
	\frac{\partial \log L_s }{\partial \vtheta}=\sum_{i=1}^N\left( \frac{\frac{1}{R}\sum_{r=1}^R\left( \frac{\partial \prod_{t=1}^{T_i} P_{itr}(\vtheta)}{\partial \vtheta}\right) }{\frac{1}{R}\sum_{r=1}^R \prod_{t = 1}^{T_i}P_{itr}(\vtheta)}\right) 
\end{equation}

For the derivative term,


\begin{equation}\label{eq:dprod_pir}
	\frac{\partial \prod_{t = 1} ^ {T_i}P_{itr}(\vtheta)}{\partial \vtheta}= P_{ir}(\vtheta)\sum_{t = 1}^{T_i}\vg_{itr}(\vtheta)
\end{equation}

where we use the fact that $\frac{\partial \log p}{\partial \theta}=\frac{1}{p}\frac{\partial p}{\partial \theta}$ and $\vg_{itr} = \partial \log P_{itr}(\vtheta) / \partial \vtheta$. Now, inserting (\ref{eq:dprod_pir}) into (\ref{eq:grad1}) we get:

\begin{equation}\label{eq:grad2}
	\frac{\partial \log L_s }{\partial \vtheta}=\sum_{i=1}\left( \frac{\sum_{r=1}^R P_{ir}(\vtheta)\vg_{ir}(\vtheta)}{\sum_{r=1}^RP_{ir}(\vtheta)}\right). 
\end{equation}

where $\sum_{t = 1}^{T_i}\vg_{itr}(\vtheta) = \vg_{ir}$. Define the weight $Q_{ir}(\vtheta)=P_{ir}(\vtheta)/ \sum_{r=1}^R P_{ir}(\vtheta)$ so that $0<Q_{ir}(\vtheta)<1$ and $\sum_{r=1}^RQ_{ir}(\vtheta)=1$. Then,

\begin{equation}\label{eq:grad3}
	\frac{\partial \log L_s }{\partial \vtheta}=\sum_{i=1}^N\sum_{r=1}^R Q_{ir}(\vtheta)\vg_{ir}(\vtheta)=\sum_{i=1}^N\bar{\vg}_i(\vtheta).
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{exam}[Gradients formulas used by \pkg{Rchoice}]
  For binary models, taking first derivatives on the $\log$ of equation (\ref{eq:probability_binary}) we get:
  
  \begin{equation}
      \frac{\partial  \log P_{itr}(\vtheta) }{\partial \vtheta}=\vg_{itr}=\lambda_{itr}(\vtheta)\left[ \frac{\partial U_{itr}^*(\vtheta)}{\partial \vtheta}\right],
   \end{equation}
   
   where:
   
   \begin{equation*}
      \lambda_{itr}=\frac{q_{it}\cdot f\left[q_{it} \cdot U_{itr}^*(\vtheta)\right]}{F\left[q_{it} \cdot U_{itr}^*(\vtheta)\right]},
   \end{equation*}
   
  and $f=\frac{\partial F(\cdot)}{\partial \vtheta}$.  If the model is the Poisson regression model model, then  $\vg_{itr}$ is given by:
  
    \begin{equation*}
      \frac{\partial  \log P_{itr}(\vtheta) }{\partial \vtheta}=\vg_{itr}=\left[y_{it}-\exp(U_{itr}^*(\vtheta))\right]\left( \frac{\partial U_{itr}^*(\vtheta)}{\partial \vtheta}\right).
   \end{equation*}
   
   For the ordered model, let $\vtheta$ the vector collecting all the parameters except for the thresholds parameters. Then, $\vg_{itr}$ is given by:
   
  \begin{equation*}
      \frac{\partial  \log P_{itr}(\vtheta) }{\partial \vtheta}=-\frac{f_{itr,j}(\vtheta)-f_{itr,j-1}(\vtheta)}{F_{itr,j}(\vtheta)-F_{itr,j-1}(\vtheta)}\left( \frac{\partial U_{itr}^*(\vtheta)}{\partial \vtheta}\right),
   \end{equation*}
   
   and by:
   
   \begin{equation*}
  	\frac{\partial \log P_{itr}(\vtheta)}{\partial \valpha_k}=\frac{d_{j,k}f_{itr,j}(\vtheta)-d_{j-1,k}f_{itr,j-1}(\vtheta)}{F_{ir,j}(\vtheta)-F_{itr,j-1}(\vtheta)}\left(\frac{\partial \vkappa_j}{\partial \valpha_k}-\frac{\partial \vkappa_{j-1}}{\partial \valpha_k}\right)
   \end{equation*}
  
  with $\delta_{j,k}=1$ if $j=k$ and 0 otherwise. Finally, if parameters are uncorrelated, then:
  
  \begin{equation*}
\frac{\partial U_{itr}^*(\vtheta)}{\partial \vtheta}=\begin{pmatrix} \vz_{it} \\ \vx_{it} \\ \vs_{i}\otimes\vx_{it}  \\ \vomega_{ir} \bullet \vx_{it}\end{pmatrix}.
\end{equation*}
  
\end{exam}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


The \pkg{Rchoice} uses this formulas to compute the gradient and uses the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm (the default) to iteratively solve the MSL.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Drawing from Densities}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The SML procedure requires drawing pseudo-random numbers from the specified distribution for simulation. A good performance of SML requires very large number of draws. As explained above, the main drawback to this approach is that with large samples and complex models, the maximization of $\log L_s$ can be very time consuming. Researchers have gained speed with no degradation in simulation performance through the use of small number of Halton draws  \citep{bhat2001quasi, train2000halton}. The idea is that, instead of taking independent random draws, simulation can potentially be improved by selecting evaluation points more systematically and with better coverage \citep{sandor2004quasi}. In this section, we detail how draws are computed by \pkg{Rchoice}. \\

Suppose that there are $K$ random parameters. Then, the $K$ elements of $\vomega_{ir}$ are drawn as follows. We begin with a $K$ random vector $\vomega_{ir}$, that is:

\begin{itemize}
  \item $K$ independent draws from the standard uniform $(0,1)$ distribution or
  \item $K$ independent draws from the $m$th Halton sequence, where $m$ is the $m$th prime number in the sequence of $K$ prime numbers beginning with 2.
\end{itemize}

An important attribute of the Halton values is that they are also distributed in the $(0,1)$ interval. Then, the primitive draw (Pseudo or Halton draws) is then transformed to the distribution specified by the user as follows: 

\begin{itemize}
  \item $u_{k,ir}\sim U(0,1)$: primitive draw from halton or pseudo-random number generator
  \item $w_{k,ir}=\Phi^{-1}(u_{k,ir})\sim N(0,1)$
\end{itemize}

Using these two primitive draws, \pkg{Rchoice} creates the random parameters as follows: 


\begin{enumerate}
\item Normal Parameter:

  \begin{eqnarray*}
    \beta_{k,ir}&=&\beta_k+\sigma_kw_{k,ir}\\
        w_{k,ir}&\sim&N(0,1)
    \end{eqnarray*}
    
    where $\beta_k$ and $\sigma_{k}$ are estimated. Then, $\beta_{k,i}\sim N(\vbeta_k,\sigma_k^2)$
    
\item  Truncated normal Parameter:   
 \begin{eqnarray*}
\beta_{k,ir}&=&
  \begin{cases}
  	\beta_k+\sigma_kw_{k,ir} & \mbox{if} \;\;\; \beta_{k,ir}>0 \\
		0 & \mbox{otherwise}
	\end{cases}\\
w_{k,ir}&\sim&N(0,1)
\end{eqnarray*}

where $\beta_k$ and $\sigma_{k}$ are estimated. Then, $\beta_{k,i}\sim N(\vbeta_k,\sigma_k^2)$ with the share below zero massed at zero

\item Log-Normal Distribution:

\begin{eqnarray*}
\beta_{k,ir}&=&\exp\left( \beta_k+\sigma_kw_{k,ir}\right)  \\
w_{k,ir}&\sim&N(0,1)
\end{eqnarray*}

where $\beta_k$ and $\sigma_{k}$ are estimated. Then, $\beta_{k,i}\sim \log N(\beta_k, \sigma_k^2)$

\item Uniform:

\begin{eqnarray*}
	\beta_{k,ir}&=&\beta_k+\sigma_k(2\times u_{k,ir}-1) \\
	u_{k,ir}&\sim& U(0,1)
\end{eqnarray*}

where $\beta_k$ and $\sigma_{k}$ are estimated. 

\item Triangular distribution:

\begin{eqnarray*}
	\beta_{k,ir}&=&\beta_k+\sigma_kv_{k,ir} \\
	v_{k,ir}&\sim& 1(u_{k,ir}<0.5)\left( \sqrt{2u_{k,ir}}-1\right)+1(u_{k,ir}\geq 0.5)\left( 1-\sqrt{2(1-u_{k,ir})} \right) 
\end{eqnarray*}

where $\beta_k$ and $\sigma_{k}$ are estimated. 

\item Johnson's S$_b$:

  \begin{eqnarray*}
      \beta_{k,ir}&=&\frac{\exp\left( \beta_k+\sigma_kw_{k,ir}\right)}{1 + \exp\left( \beta_k+\sigma_kw_{k,ir}\right)}  \\
w_{k,ir}&\sim&N(0,1)
  \end{eqnarray*}
  
  where $\beta_k$ and $\sigma_{k}$ are estimated.
\end{enumerate}

In applied work, one issue that causes concern is the choice of the distribution of the random parameters. As explained by \cite{hensher2003mixed}, distributions are essentially arbitrary approximations to the real behavioral profile. The researcher choose a specific distribution because he has a sense that the ``empirical truth'' is somewhere in their domain. For example, the log-normal form is often used if the coefficient needs to be specific non-negative sign. On the contrary, the normal and triangular distributions are useful when there is no certainty of the sign of the coefficient. The problem with the normal distribution is that its domain is $(-\infty, \infty)$ which may results in very extreme coefficients. The triangular distribution may solve this problem because it possesses shorter tails. \\ 

\pkg{Rchoice} allows to the user to specify two type of random draws by the argument \code{haltons}: pseudo-random draws (\code{haltons = NULL}) and Halton draws (\code{haltons = NA}) as default. If \code{haltons = NULL}, the seed is set to \code{set.seed(123)}. The user can change this by the \code{seed} argument. For the Halton draws, the default is to use the first $K_2$ primes numbers starting with 3. Within each series, the first 100 draws are discarded, as the first draws tend to be highly correlated across different draws. The user can also change the prime  number and the element dropped for each serie. For example, if $K_2=2$, and the user wants to use the primes numbers 5 and 31 along with dropping the first 10 draws, he could specify \code{ haltons = list("prime" = c(5,31), "drop" = c(10,10))}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Applications Using Rchoice}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Standard Models}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, we show the capabilities of \pkg{Rchoice} to estimate Poisson, Binary and Ordered regression model without random parameters.\footnote{In \proglang{R} there exist several package to estimate binary, count and ordered models. \code{glm} function allows to estimate different kind of discrete choice models such as Poisson and binary models. The function \code{probit} from the package \pkg{micEcon} allows to estimate probit model. Moreover, the function \code{polr} from the package \pkg{MASS} allows to estimate ordered models \citep{venablesbook}. The advantage of \pkg{Rchoice} is that allows more flexibility in the optimization routines which improves the convergence speed.  \pkg{Rchoice} uses the function \code{maxLik} in order to maximize the log-likelihood function, which permits to estimate models by the Newton-Raphson (NR), BGFS and Berndt-Hall-Hall-Hausman (BHHH) procedures \citep[see][]{henningsen2011maxlik}.} The main objective of this section is to show how \pkg{Rchoice} can interact with other packages in \proglang{R}. \\

\pkg{Rchoice} is loaded by typing: 

<<message = FALSE>>=
library("Rchoice")
@

To show how to estimate Poisson regression models using \pkg{Rchoice}, we will use data on scientific productivity \citep{long1990origins, long1997regression}. We load the data using

<<>>=
data("Articles")
head(Articles,3)
@

To see more information about the data, one can use:

<<eval=FALSE>>=
help(Articles)
@

The work by \cite{long1990origins} suggests that gender, marital status, number of young children, prestige of the graduate program, and the number of articles written by a scientist's mentor could affect a scientist's level of publication. To see this, we estimate a Poisson regression model and use the \code{Rchoice} function specifying \code{link = poisson}:

<<>>=
poisson <- Rchoice(art ~ fem + mar + kid5 + phd + ment, 
                   data = Articles, 
                   family = poisson)
summary(poisson)
@

The output shows that the log-likelihood function is estimated using NR algorithm in 7 iterations. If the user wants to estimate the model using another algorithm he should type \code{method = "bfgs"} for the BGFS method or \code{method = "bhhh"} for BHHH method.\footnote{BHHH is generally faster than the other procedures, but it can blow up if the variables have very different scale. The larger the ratio between the largest standard deviation and the smallest standard deviation, the more problems the user will have with the estimation procedure. Given this, we encourage the users check the variables and re-scale or recode them if necessary. \code{Rchoice} uses the numerical hessian if \code{method = 'nr'} and the model is estimated with random parameters, thus it can be very slow compare to the other methods.}\\

In terms of interpretation, we can say that, being a female scientist decreases the expected number of articles by a factor of 0.8 $(=\exp(-.225))$, holding all other variables constant. Or equivalently, being a female scientist decreases the expected number of articles by 20\% $(=100\left[\exp(-.225)-1\right])$, holding all other variables constant. Prestige of PhD department is not important for productivity. \\

Another capability of \pkg{Rchoice} is its interaction with other packages in \proglang{R}. For example, we can compute the robust standard error by using the package \pkg{sandwich}:

<<message=FALSE>>=
library(sandwich)
library(lmtest)
coeftest(poisson, vcov = sandwich)
@

To get the same robust standard errors as STATA \citep{stata12}, we need to make a small sample correction:

<<>>=
vcov.stata <- vcovHC(poisson, type = "HC0") * nObs(poisson)/(nObs(poisson)-1)
coeftest(poisson, vcov = vcov.stata)
@

where the correction is $n/(n-1)$. \\

\pkg{Rchoice} also interacts with \code{linearHypothesis} and \code{deltaMethod} functions from \pkg{car} \citep{fox2009car} and the  \code{lrtest} and \code{waldtest} functions from \pkg{lmtest} package \citep{zeileisR}. For example, we can test $H_0: \texttt{phd}/\texttt{ment}=0$ by: 

<<message=FALSE, warning = FALSE>>=
library(car)
deltaMethod(poisson, "phd/ment")
@

The main argument to estimate other models is \code{family}. For probit models, the user should specify \code{family = binomial("probit")},  and for Logit \code{family = binomial("logit")}. In the following example, we use the \code{Workmroz} data base to estimate a binary Probit model, where the dependent variable \code{lfp} equals 1 if wife is in the paid labor force, and 0 otherwise.

<<>>=
data("Workmroz")
probit <- Rchoice(lfp ~ k5 + k618 + age + wc + hc + lwg + linc,
                  data = Workmroz,
                  family = binomial('probit'))
summary(probit)
@


Ordered Probit and Logit models are estimated in the same way. In this case we use \code{Health} database and create the logarithm of household income. The dependent variable, \code{newhsat}, is a categorical variable that indicates the self reported health assessment of individuals recorded with values 0,1,...,4.

<<>>=
data("Health")
Health$linc <- log(Health$hhinc)
ologit <- Rchoice(newhsat ~ age + educ + married + hhkids + linc,
                   data = Health[1:2000, ],
                   family = ordinal('logit'))
summary(ologit)
@



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Random Parameters Models with Cross Sectional Data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The main advantage of \pkg{Rchoice} over other packages is that it allows estimating models with random parameters.  In this section, we show how to estimate those kinds of models for Binary, Ordered and Poisson models using cross-sectional data. \\ 

First, we estimate a Poisson regression model with random parameters. In this case, we will assume that the effect of \texttt{kid5}, \texttt{phd} and \texttt{ment} are not fixed, but rather heterogeneous across the population. Specifically, we will assume that the coefficients for those variables are independent normally distributed, that is, we will not allow correlation among them:

\begin{eqnarray*}
 \beta_{\texttt{kid5},i}&=&\beta_{\texttt{kid5}}+\sigma_{\texttt{kid5}}\omega_{\texttt{kid5},ir} \\
 \beta_{\texttt{phd},i}&=&\beta_{\texttt{phd}}+\sigma_{\texttt{phd}}\omega_{\texttt{phd},ir} \\
 \beta_{\texttt{ment},i}&=&\beta_{\texttt{ment}}+\sigma_{\texttt{ment}}\omega_{\texttt{ment},ir}   
\end{eqnarray*}

where $\omega_{k,ir}\sim N(0, 1)$. Then, in order to estimate this model, we can write:

<<>>=
poisson.ran <- Rchoice(art ~ fem + mar + kid5 + phd + ment, 
                       data = Articles,  
                       family = poisson,
                       ranp = c(kid5 = "n", phd = "n", ment = "n"))
@

It is important to discuss the arguments for the \code{Rchoice} function. First, the argument \code{ranp} indicates which variables are random in the \code{formula} and their distributions. In this case, we have specified that all of them are normal distributed using \code{"n"}. The rest of the distributions are:

\begin{itemize}
  \item Normal  = \code{"n"},
  \item Log-Normal = \code{"ln"},
  \item Truncated Normal = \code{"cn"},
  \item Uniform = \code{"u"},
  \item Triangular = \code{"t"},
  \item Johnson's S$_b$ = \code{"sb"}
\end{itemize}



The number of draws are not specified. Therefore, \code{Rchoice} will set \code{R = 40} as default. The user can change this by changing the \code{R} argument. The type of draws are Halton draws default, but if the user wants pseudo-random draws he can specify \code{haltons = NULL}. Finally, it is always important checking the exit of the estimation. As explained before, the default maximization algorithm for SML is BGFS.\footnote{For more information about arguments for the optimization type \code{help(maxLik)}.} 

<<>>=
summary(poisson.ran)
@

It is important checking the exist of the estimation. In our example, the output informs us that the convergence was achieved successfully. The results also show that the standard deviations of the coefficients are highly significant, indicating that parameters do indeed vary in the population. Since the parameters are normally distributed, we can also say that : 

<<>>=
pnorm(coef(poisson.ran)["mean.kid5"]/coef(poisson.ran)["sd.kid5"])
@

a 24\% of the individuals have a positive coefficient for \texttt{kid5}. In other words, for about 76\% of PhD students, having children less than 6 years old reduces their productivity. Note also that the mean coefficient for \texttt{phd} is 0 (not significant). This is due to the fact that the unobserved heterogeneity among scientists in the sample cancel out positive and negative effects. These observations are not possible with a Poisson regression with fixed parameters. \\

Suppose that now we want to test if $H_0=\sigma_{\texttt{kid5}}=\sigma_{\texttt{phd}}=\sigma_{\texttt{ment}}=0$. This can be done by using the function \code{waldtest} or \code{lrtest} from package \pkg{lmtest}:

<<>>=
waldtest(poisson.ran, poisson)
lrtest(poisson.ran, poisson)
@

Both tests reject the null hypothesis. We can also specify different distribution of the parameters by using the S3 method \code{update}:

<<eval = FALSE>>=
poisson.ran2 <- update(poisson.ran, 
                       ranp = c(kid5 = "u", phd = "t" , ment = "cn"), 
                       R = 10)
@

Both models \code{poisson.ran} and \code{poisson.ran2} can be compared using \code{mtable} from \pkg{memisc}:

<<echo=TRUE, message=FALSE>>=
library(memisc)
@

<<eval = FALSE>>=
mtable("model 1"= poisson.ran, "model 2" = poisson.ran2, 
       summary.stats = c("N", "Log-likelihood", "BIC", "AIC")) 
@

The previous model specifies the coefficients to be independently distributed while one would expect correlation. For example, the effect of the prestige of PhD department could be positive correlated with the number of publications by mentor. Now, we estimate the model \texttt{poisson.ran}, but assuming that the random parameters are correlated. That is, we specified  $\vbeta_i \sim N(\vbeta, \mSigma)$ for general $\mSigma$:

<<>>=
poissonc.ran <- Rchoice(art ~ fem + mar + kid5 + phd + ment, 
                        data = Articles, 
                        ranp = c(kid5 = "n", phd = "n", ment = "n"), 
                        family = poisson, 
                        correlation =  TRUE,
                        R = 10)
summary(poissonc.ran)
@

The output prints the mean of the random parameters along with the lower-triangular Choleski factor $\mL$. We can extract the $\mSigma=\mL\mL'$ matrix of variance-covariance matrix and the correlation matrix of the random parameters using \code{cov.Rchoice} and \code{cor.Rchoice}:

<<>>=
cov.Rchoice(poissonc.ran)
cor.Rchoice(poissonc.ran)
@

Among other things, the output shows that the three parameters are positively correlated. Specifically, we can see that the correlation between \code{phd} and \code{ment} is around -0.4. We can also test if the variances of the random parameters are significant using Delta Method. For this, we can use the \code{se.cov.Rchoice} function, which is a wrapper of \code{deltamethod} function from \pkg{msm} package. For example:

<<>>=
se.cov.Rchoice(poissonc.ran)
@

To get the standar errors of the standard deviations for the random parameters, we might use:

<<>>=
se.cov.Rchoice(poissonc.ran, sd = TRUE)
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Random Parameters Models with Panel Data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This new version of this package also handles panel data by estimating Random Effect Models (RE). The RE model specifies:

\begin{equation*}
  \epsilon_{it} = \upsilon_{it} + u_i
\end{equation*}

where $\upsilon_{it}$ and $u_i$ are independent variables with

\begin{eqnarray*}
  \E(\upsilon_{it} | \mX) = 0; \cov(\upsilon_{it}, \upsilon_{is}| \mX) = \var(\upsilon_{it}|\mX) = 1, &\;\;& \mbox{if}\;\; i =j\;\;\mbox{and}\;\; t = s; 0 \;\mbox{otherwise},\\
\E(u_i| \mX) = 0, \cov(u_i, u_j | \mX) = \var(u_i| \mX) = \sigma_u^2, &\;\;& \mbox{if}\;\; i = j;\;\;0 \;\mbox{otherwise}, \\
\cov(\upsilon_{it}, u_j) = 0 &\;\;& \mbox{for all}\;\; i,t,j,
\end{eqnarray*}

and $\mX$ indicates all the exogenous data in the sample, $\vx_{it}$ for all $i$ and $t$. This model can be estimated using the package \pkg{pglm} \citep{croissantpglm}, which uses \textbf{quadrature} to approximate the integration in the probability.\\

\pkg{Rchoice} also allows estimating RE effect models along with random parameters. Note that assuming that the constant is random is equivalent to a RE model. Therefore, the user might estimate a simple RE model by typing \code{ranp = (constant = 'n')}.\\

In this example we estimate a Probit Model with RE and random parameters using \code{Unions} database from the \pkg{pglm} package. 

<<>>=
data('Unions', package = 'pglm')
Unions$lwage <- log(Unions$wage)
@

The model is estimated using the following syntax:

<<>>=
union.ran <- Rchoice(union ~ age + exper + rural + lwage,
                     data = Unions[1:2000, ],
                     family = binomial('probit'),
                     ranp = c(constant = "n", lwage = "t"),
                     R = 10,
                     panel = TRUE,
                     index = "id",
                     print.init = TRUE)
@

In this case, we assume that \code{lwage} is distributed as triangular, while the constant is assumed to be normal distributed. This is the same as assuming that $u_i \sim \rN(0, \sigma_u^2)$.\\

There are two main arguments for the panel estimation. The argument \code{panel = TRUE} indicates that the data is a panel. This implies that the user should indicate the variable that corresponds to the ID of the individuals in the \code{index} argument.\\

Finally, the argument \code{print.init = TRUE} indicates that the initial values used by \code{Rchoice} will be displayed.

<<>>=
summary(union.ran)
@

The results indicate that $\sigma_u =$\Sexpr{coef(union.ran)["sd.constant"]} and is significant. On the other hand, the finding of a significant standard deviation yet insignificant mean for \code{lwage} attests to the existence of substantial heterogeneity; positive and negative coefficient in the sample compensate for each other, such that the coefficient on the mean is not significant. \\

As in the previous cases, an Ordered Probit Model with RE and random parameters can be estimated in the same way, but changing the distribution with the \code{family} argument. 

<<eval = FALSE>>=
oprobit.ran <- Rchoice(newhsat ~ age + educ + married + hhkids + linc,
                      data = Health[1:2000, ],
                      family = ordinal('probit'),
                      ranp = c(constant = "n", hhkids = "n", linc = "n"),
                      panel = TRUE,
                      index = "id",
                      R = 100,
                      print.init = TRUE)
summary(oprobit.ran)
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hierarchical Poisson Random Parameter Model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section we show how to estimate a Hierarchical Poisson Random Parameter Model. In this case, we assume that there exist not only unobserved heterogeneity in the coefficients for \texttt{phd} and \texttt{ment}, but also observed heterogeneity in the mean of the coefficients. Specifically, we assume that:

\begin{eqnarray*}
 \beta_{\texttt{kid5},ir}&=&\beta_{\texttt{kid5}}+\sigma_{\texttt{kid5}}\omega_{\texttt{kid5},ir} \\
 \beta_{\texttt{phd},ir}&=&\beta_{\texttt{phd}}+\pi_{\texttt{phd}, \texttt{fem}}\texttt{fem}+ \sigma_{\texttt{phd}}\omega_{\texttt{phd},ir} \\
 \beta_{\texttt{ment},ir}&=&\beta_{\texttt{ment}}+\pi_{\texttt{ment}, \texttt{fem}}\texttt{fem}+\pi_{\texttt{ment}, \texttt{phd}}\texttt{phd}+\sigma_{\texttt{ment}}\omega_{\texttt{ment},ir}   
\end{eqnarray*}

The formulation above implies that, for example, \code{ment}'s coefficient (or marginal effect on latent productivity) varies by gender and \code{phd}. 

<<eval = FALSE>>=
poissonH.ran <- Rchoice(art ~ fem + mar + kid5 + phd + ment | fem + phd,
                        data = Articles,
                        ranp = c(kid5 = "n", phd = "n", ment = "n"),
                        mvar = list(phd = c("fem"), ment = c("fem", "phd")),
                        family = poisson,
                        R = 10)
@

\pkg{Rchoice} manages the variables in the hierarchical model by the \code{formula} object. Note that the second part of the \code{formula} is reserved for all the variables that enter in the mean of the random parameters. The argument \code{mvar} is a list that indicates how all this variables enter in each random parameter. For example \code{phd = c("fem")}  indicates that the mean of \code{phd} coefficient varies according to \code{fem}. 

<<eval = FALSE>>=
summary(poissonH.ran)  
@

We can test if the interaction variables are jointly significant by using \code{lrtest}:

<<eval = FALSE>>=
lrtest(poissonH.ran, poisson.ran)
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Plotting Conditional Means}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

It is important to note that the estimates of the model parameters provide the unconditional estimates of the parameter vector, but we can form a person specific conditional estimator \citep[see][]{train2009discrete, greenebook}. The estimator of the conditional mean of the distribution of the random parameters, conditioned on the person specific data, is:

\begin{equation*}
\widehat{\bar{\vbeta_i}}=\widehat{E}(\vbeta_i|\mbox{data}_i)=\sum_{r=1}^R\widehat{Q}_{ir}\widehat{\vbeta}_{ir}
\end{equation*}
where:

\begin{equation*}
  \widehat{\vbeta}_{ir}=\widehat{\vbeta}+\widehat{\mPi}\vs_i+\widehat{\mL}\vomega_{ir}
\end{equation*}

Note that these are not actual estimates of $\vbeta_i$, but are estimates of the conditional mean of the distribution of the random parameters \citep{greene2014estimating}. We can also estimate the standard deviation of this distribution by estimating:

\begin{equation*}
\widehat{E}(\vbeta_i^2|\mbox{data}_i)=\sum_{r=1}^R\widehat{Q}_{ir}\widehat{\vbeta}_{ir}^2,	
\end{equation*}

and then computing the square root of the estimated variance,

\begin{equation*}
	\sqrt{\widehat{E}(\vbeta_i^2|\mbox{data}_i)-\widehat{E}(\vbeta_i|\mbox{data}_i)^2}
\end{equation*}

With the estimates of the conditional mean and conditional variance, we can then compute the limits of an interval that resembles a confidence interval as the mean plus and minus two estimated standard deviation. This will construct an interval that contains at least 95 percent of the conditional distribution of $\vbeta_i$ \citep{greenebook}.\\ 

\pkg{Rchoice} allows to plot the histogram and kernel density of conditional means of random parameters using the function \code{plot}. For instance, the kernel of the conditional mean of $\beta_{\texttt{lwage},i}$ for \code{union.ran} model can be obtained by typing:

<<eval =  FALSE>>=
plot(union.ran, par = "lwage", type = "density")
@


\begin{figure}[H]
  \centering
   \caption{Kernel Density of the Individual's Conditional Mean}
<<plot1, echo = FALSE>>=
plot(union.ran, par = "lwage", type = "density")
@
\end{figure}

As \cite{greenebook} points out, even if the analysis departs from normal marginal distributions $\vbeta_i$, the sample distribution of the $n$ estimated conditional means is not necessarily normal. Therefore, the kernel estimator based on the $n$ estimators can have a variety of shapes.\\

We may also plot the individual confident interval for the conditional means for the first 20 individuals:

<<eval =  FALSE>>=
plot(union.ran, par = "lwage", ind =  TRUE, id = 1:20, col = "blue")
@

\begin{figure}[H]
  \centering
   \caption{Individual Confident Interval for the Conditional Means}
<<plot2, echo = FALSE>>=
plot(union.ran, par = "lwage", ind =  TRUE, id = 1:20, col = "blue")
@
\end{figure}

The method \code{plot} for \code{Rchoice} class is a wrapper of \code{effect.Rchoice} function. This function is a helper function to obtain the conditional estimate of the individual random parameters or the compensating variations.

<<>>=
bi.wage <- effect.Rchoice(union.ran, par = "lwage", effect = "ce")
@

The argument \code{effect} is a string indicating what type of effect should be computed. In this example, we are requiring the  the conditional expectation of the individual coefficients \code{"ce"}. \code{effect.Rchoice} returns two list. The first one with the estimated conditional means for all the individuals, and the second one with the estimated standard errors of the conditional means.

<<>>=
summary(bi.wage$mean)
summary(bi.wage$sd.est)
@

One might also get the individuals' \textbf{compensating variations} using both \code{plot} and \code{effect.Rchoice}. Compensating variation is the variation in two regressors such that the latent variables does not change. Let $x_{il}$ denote the $l$th elment in $\vx_i$ and $\beta_l$ the corresponding parameter, and let $m$ index the $m$th elements in both vectors $\vx_i$ and $\vbeta$, respectively. Now consider a change in $x_{il}$ and $x_{im}$ at the same time, such that $U^*_{i} = 0$. This requires

\begin{equation*}
  0 = \beta_l\Delta x_{il} + \beta_{im} \Delta x_{im} \implies \frac{\Delta x_{il}}{\Delta x_{im}} = -\frac{\beta_{im}}{\beta_l}
\end{equation*}

where $\beta_{im}$ is a random coefficient. This ratio (without the minus sign) is computed or plotted if the argument \code{effect = "cv"} in any of the two functions. The argument \code{par} is the variable whose coefficient goes in the numerator ($\beta_{im}$), and the argument \code{wrt} is a string indicated which coefficient goes in the denominator ($\beta_l$). Note that since $\beta_{im}$ is random, the ratio of the coefficient is random and its distribution follows from the joint distribution of the coefficients.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and future development}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The \pkg{Rchoice} package contains most of the newly developed models in binary, count and ordered models with random parameters. \pkg{Rchoice} handles cross-section data with observed and unobserved heterogeneity. Allowing parameter values to vary across the population according to some pre-specified distribution overcomes the problem of having a fixed-representative coefficient for all individuals. The distribution supported by \pkg{Rchoice} are normal, log-normal, uniform, truncated normal and triangular distribution. It also allows to the user choose between Halton draws and pseudo-random numbers and correlated parameters.\\

The \pkg{Rchoice} package intends to make available those estimation methods to the general public and practitioners in a friendly and flexible way. In future versions, we expect to add functions that allows estimating latent class models. We also hope to include functions to compute marginal effects. 


%\pagebreak
\bibliography{Rchoice.bib}


\end{document}